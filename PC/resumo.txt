================================
UNIDADE 1
================================
Introdução à Programação concorrente
=> concorrência: múltiplas tarefas logicamente ativas em determinado momento
	-> alterandno entre execução das tarefas
	-> não executa ao mesmo tempo
=> paralelismo:  múltiplas tarefas realmente sendo executadas ao mesmo tempo
	-> concorrência + hardware paralelo + desempenho 
	-> se é paralelo, é concorrente

=> mais fácil para expressar certos problemas
=> melhor uso dos recursos computacionais
=> executar aplicações em menos tempo ou melhorar precisão nos resultados sem impactar no tempo de execução

=> debug mais difícil (comportamento não determinístico)
=> controle de concorrência (evitar alteração de dado ao mesmo tempo e ordem de execução)
=> corretudo mais difícil (não determinismo)

=> exclusão mútua: garantir que duas tarefas do programa não alterem um mesmo dado ao mesmo tempo
	-> regiões críticas
=> sincronização: garantir ordem de execução das tarefas

=> dependêncida de dados, sua distribuição, sincronização, ...	= problemas

=> C, C#, python, java, haskell, Erlang, Clojure, Elixir, Go, Ruby, Rust, ...

-----------------------------------

Plataformas de execução

- Multiprocessadores
=> todos processadores acessam uma memória compartilhada
=> comunicação por compartilhamento de memória
=> multicoress (CMP = chip multiprocessing)
=> múltiplos processadores em uma mesma máquina (cada um em um socket)

=> 2 formas de organizar a memória:
	=> Uniform Memory Access (UMA)
		-> memória centralizada
		-> acessos levam o mesmo tempo
			-> latência e largura de banda padrão; previsibilidade
		-> Todos os processadores ligados à uma interconexão, que esta está ligada à uma memória
	=> Non-uniform Memory Access (NUMA)	
		-> memória distribuída em módulos
		-> tempo de acesso depende se a memória local ou remota
		-> há espaço de endereçamento, composto por memórias isoladas entre si, cada uma ligada à um processador e estes à uma interconexão

- Aceleradores: usados junto de CPUs para acelerar computação
=> GPUs, multicores, manycores, FPGAs, ...

- Multicomputadores
=> NORMA: NO Remote Memory Access; computadores inteiros replicados, cada um com sua memória e comunicação por rede
=> classificação:
	=> NOW: Network of workstations
		-> desktops interligados; rede tradicional de baixo custo; ambientes oportunistas
	=> COW: Cluster of workstatios
		-> NOW para alto desempenho; redes de baixa latência; componentes redundantes; top500
	=> Grid/Grade
		-> máquina composta por componentes distribuídos geograficamente; controle descentralizado; interfaces e protocolos padronizados
	=> Cloud
		-> similar a computação de grid; recursos são usados sob demanda (escalável e pago com uso); recursos virtualizados

-----------------------------------

Extração de concorrência (exemplo prático)

=> Double-Precision A.X Plus Y (DAXPY)
	-> dois vetores, x e y, de números em precisão dupla e um escalar "a", calcula y = ax+y

#define T 100000
#define a 42
...
	double x[T], y[Y];
	int i;
	for (i = 0; i < T; i++)
		y[i] = a*x[i] + y[i]

=> tarefa = conjunto de instruções + dados
=> pode-se criar concorrência criando tarefas e dando à elas valores para calcular do for ou um valor

=> criação de tarefas: por abstrações básicas do SO e/ou linguagem como processos e threads

-----------------------------------

Noções de Análise de desempenho

=> Tempo(n): tempo de execução usando n núcleos
=> Speedup(n): ganho de desempenho da execução paralela com n núcleos de processamento sobre o tempo de execução sequencial
	Speedup(n) = Tempo(1) / Tempo(n)
=> Eficiência(n): aproveitamento da plataforma
	Eficiência = Speedup(n) / n

=> Lei de Amdahl: representa ganho máximo teórico de desempenho de programa paralelo
	-> divide algoritmo em parcela serial e outra paralela
	-> parcela serial: f entre 0 e 1
	-> parecela paralela: 1 - f
	-> núcleos/num. processadores: n
=> cálculo do tempo teórico segundo essa lei: 
	parcela serial: f e [0; 1]
	número de núcleos: n
	
	Tempo(n) = Tempo(1) * f + Tempo(1) * (1-f)/n
		=> tempo(1) * f = serial
		=> Tempo(1) * (1-f)/n = paralela
	Tempo(n) = Tempo(1) * (f + (1-f)/n)

	Speedup(n) = 1 / (f + (1-f)/n)

-----------------------------------

Introdução a linguagem C

char: %c
int : %d
unsigned int: %u
unsigned long int: %lu
float: %f
double: %g
char* (string): %s

=> função que recebe matriz, precisa ao menos do valor da segunda dimensão especificado na declaração
=> strings = array de char
	-> terminam em '\0' => def. fim da string
	-> tamanho para armazenar o texto mais NUL

=> string.h:
	strlen() => tamanho da string
	strcpy() => copia string para outra
	strcat() => concatena duas strings
	strcmp() => compara duas strings
=> stdlib.h:
	atoi(str) => converte string para int

=> malloc(): aloca área contígua de memória e retorna ponteiro para início da área (NULL caso erro)
	type* ptr = (type *) malloc(byte-size)
	-> type = tipo do dado a alocar
	-> byte-size = quantidade de bytes a aserem alocados (múltiplo do tipo de dados)
	int* array_int = (int *) malloc(100 * sizeof(int))	=> arrray de 100 inteiros
=> free(): desaloca área alocada por malloc()
	free(ptr);

typedef struct {
	...
} t_novo_tipo


================================
UNIDADE 2
================================

Processos

=> programa: entidade estática e permanente; sequência de instruções; passivo sob ponto de vista do SO
=> processo: abstração que representa um programa em execução; entidade dinânica (altera estado ao longo da execução)
	-> programa (código), dados e contexto (valores; registradores)
	-> podem ser:
		instâncias de programas diferentes
		diferentes instâncias de um mesmo programa
=> quando instâncias de programas diferentes: dois programas diferentes sendo executados pelo processador
	-> "instancia de um navegador e instancia de notepad"
=> quando diferentes instancias de mesmo programa: mesmo código (programa) mas com dados e momentos de execução (contextos) diferentes
	-> mesmo programa pode ter várias instancias em execução
	-> forma que SO enxerga um programa e possibilita a execução
	-> dois chromes abertos ao mesmo tempo

=> Ciclo de vida de um processo:
	-> são criados e destruídos (momento e forma depende do SO)
		-> normalmente com "chamadas de sistema"
	-> após criação, é executado pelo processador (ciclo de processador), podendo dexar para realizar I/O
=> dois tipos de processo: processos de sistema (daemons) e processo de usuário
=> perfis de processo:
	-> CPU-bound: tempo de execução do processo definido principalmente pelo tempo dos seus ciclos de processador
	-> Memory-bound: tempo de exec. def. princ. pelo tempo de acesso à memória
	-> I/O-bound: tempo def. pela duração das operações de I/O

=> processos alternam entre diferentes estados durante ciclos de vida
	-> criação: quando usuário executa programa, SO cria um processo	
		-> chamadas de sistema (fork, spawn, ...)
		-> PID (process identification)
	-> pronto: pronto para ser executado pela CPU
		-> há fila de processos prontos, da qual SO escolhe (seleção)
		-> escalonador: do SO, seleciona processo pronto (depende do algoritmo de escalonamento e com critérios diferentes (tempo de uso da CPU, prioridade)
	-> em execução: executado pela CPU
		-> fica em execução até ser preemptado -> volta para pronto e outro processo é selecionado
			-> preempção depende do algo. de escalonamento (limite de tempo de  CPU atingido, um com mais prioridade fica pronto, ...)
	-> bloqueado: ao realizar chamada de sistema -> armazenados em filas (uma para cada dispositivo de I/O)
		-> chamada de sistema quando processo requisita algum serviço do SO => gera interrupção após término do tratamento da requisição
	-> destruição (término): memória usada prlo processo é liberada
		-> erro, intervenção de outro processo (kill), ...

=> SO tem tabela de processos: entradas na tabela=PCB (Process Control Block, ou descritor do processo)
	-> entradas contém informações sobre contexto do processo (regs, PC, SP, estado, PID, PID do pai, diretório, estatísticas, ...)

Processos no Linux
=> fork()
	-> retorna > 0 para o processo pai (PID do filho criado)
	-> retorna   0 para o processo filho
=> processo filho é cópia do proceso pai (ambos executam o código que vier após o fork)

=> wait(NULL) permite processo aguardar término de um filho
	-> retorno:
		>= 0 : PID do processo filho que terminou
		 -1  : quando não tem mais filhos
=> para que cada processo espero o término de todos seus filhos: while (wait(NULL) >= 0);

=> código simples:
#include <unistd.h>
#include <sys/wait.h>
...
	pid_t pid;
	pid = fork();

	if (pid >= 0) {  // conseguiu criar o processo
		if (pid == 0)
			Filho();
		else {
			Pai();
			wait(NULL);  // espera o filho
		}
		return 0;
	} else // erro e não criou (< 0)
		printf(...)
	return 1

=> processos não compartilham o mesmo espaço de endereçamento
=> pai e filho possuem cópias idênticas dos dados após fork()
=> não compartilham variáveis globais, cada um possui cópia delas em seu espaço de endereçamento

=> top, htop, ps (lista processos ativos), kill, pstree

-----------------------------------

Threads e Compartilhamento de Dados
=> threads = linhas de execução independentes de um processo (por padrão, 1 processo tem 1 thread)
	-> threads de mesmo processo compartilham memória e alguns recursos do SO
=> info. mantida: PC, registradores, pilha de execução, estado
=> mais barato/rápido para gerenciar

POSIX Threads
=> POSIX = Portable Operating System Interface; define API para compatibilidade de software com variantes de Unix e outros SOs
=> SO cria processo quando inicia program -> contem uma thread (main thread), que pode criar threads filhas/trabalhadoras (que podem criar outras)
=> escalonamento: Linux usa threads em espaço de núcleo e são escalonáveis individualmente

---> pthread_create(thread, attr, start_routine, arg)
	-> identificador único retornado pela criação da thread (passa endereço)
	-> atributos (padrão NULL)
	-> rotina que executa uma vez quando thread for criada
	-> argumento para start_routine 	((void*)&argumento) -> converter de volta: tipo variavel = *((tipo*)argumento)
---> pthread_exit(retval)	-> dentro da execução da thread o faz para terminar/retornar
	-> valor a retornar para a thread pai
---> pthread_join(thread, retval)	-> na thread que criou a thread
	-> id único retornado por pthread_create(...)
	-> var. que armazena valor passado para a função pthread_exit() => NULL para não armazenar

#include <pthread.h>
#include <stdio.h>
void * print_hello(void *arg) {
	pthread_t tid = pthread_self();
	printf("Thread %u: Hello World!\n", (unsigned int) tid);
	pthread_exit(NULL);
}
int main(int argc, char **argv) {
	pthread_t thread;
	pthread_create(&thread, NULL, print_hello, NULL);
	pthread_join(thread, NULL);
return 0;
}

=> compartilhamento de dados em memória entre threads em um mesmo processo com leitura/escrita em variáveis na memória

=> condição de corrida: duas ou mais threads mudam mesmo conjunto de dados concorrentemente; resultado depende da ordem que acessos aos dados são feitos
=> reigão/seção crítica: partes do program concorrente em que tem acesso à memória compartilhada
	-> precisa proteger com mecanismo de exclusão mútua (evitando condição de corrida)
=> exclusão mútua: mecanismo de controle de concorrência que permite que apenas uma thread execute uma região crítica por vez
	-> threads que tentam acessar uma região crítica já ocupada são bloqueadas e aguardam sua vez

=> requisitos de mecanismo que impelmente exclusão mútua:
	1. duas ou mais threads nunca podem estar simultaneamente dentro de uma mesma região crítica
	2. deve funcionar para número qualquer de processadores/cores
	3. threads só são bloqueadas se estiverem tentando acessar uma mesma região crítica ao mesmo tempo
	4. nenhuma thread pode esperar eternamente para entrar em região crítica

=> mecanismos de ex. mútua: algo. de Peterson, spin-locks, mutexes
=> mec. de ex. mutua e sincronização: semáforos

-----------------------------------

Exclusão Mútua

- Algoritmo de Peterson
=> não precisa de suporte de hardware
=> FLAG[]: vetor que indica quais threads desejam acesar região crítica
	-> index = numero da thread
=> TURN: decide qual thread tem acesso em caso de disputa
=> inicialmente, todos valendo 0

Thread 0 (T0):
FLAG[0] = 1;
TURN = 1;
while (FLAG[1] == 1 && TURN == 1);  (não passa daqui e não faz nada)
// região crítica
FLAG[0] = 0;

Thread 1 (T1)
FLAG[1] = 1;
TURN = 0;
while (FLAG[0] == 1 && TURN == 0);  (não passa daqui e não faz nada)
// região crítica
FLAG[1] = 0;

=> há espera ocupada (threads esperam permissão pra entrar na região em loop de teste -> uso desnecessário do processador)
=> complexidade: fácil implementação para 2 threads, mais complexo para mais threads


- Spin-locks
=> sincronização baseada em busy waiting
=> usa instrução específica de hardware
	-> exchange (XCHG)
=> execução atômica

=> região crítica protegida por varíavel em memória ("LOCK")
	-> inicialmente em =0
	-> se LOCK = 0 -> região crítica livre
	-> se LOCK = 1 -> região crítica ocupada

=> entrando em região crítica:
	MV REG, #1	-> REG = 1
	XCHG REG, LOCK	-> troca os dois valores
	CMP REG, #0	-> se for 1, LOCK era 1 e estava ocupada
	JNE ENTRAR	-> se era 1 de fato (enquanto ocupada)
	RET		-> se era 0 e houve troca, estava livre e pode entrar na região crítica
=> saindo de região crítica:
	MV LOCK, #0	-> indica estar livre
	RET
	
=> problema: busy waiting (uso desnecessário do processador)

- Mutex:
=> proteção de região crítica
=> evita busy waiting
=> threads são bloqueadas enquanto esperam sua vez

=> tipo abstrato de dados:
	-> valor lógico (livre ou ocupado)
	-> fila de threads bloqueadas que esperam sua vez para acessar a região crítica que aquele mutex protege
=> operações sobre mutex m (atomicas; usam spin-locks para garantir):
	-> lock(m): solicita acesso à região crítica
	-> unlock(m): libera acesso a região crítica
	
lock(m):
	se m está livre:
		m = ocupado
	senao:
		bloqueia thread
		insere a thread no fim da fila do mutex m	-> não usa CPU
unlock(m):
	se fila de m está vazia:
		m = livre
	senao:
		libera thread do inicio da fila de m
	
=> apenas com spin-lock, a thread fica em looping consumindo processador esperando, tempo de uso da região crítica pode ser muito grande e desperdiçar CPU
=> com mutex, fica em loop apenas enquanto espera para executar funções lock() e unlock()
	-> se está ocupado, thread bloqueada e libera processador
	-> tempo de execução de lock e unlock é pequeno
	
---> phtread_mutex_init(&mutex, attr)
	-> mutex = variável do tipo pthread_mutex_t para inicializar
	-> attr  = atributos específicos, padrão = NULL
---> pthread_mutex_destroy(&mutex)

pthread_mutex_t mutex;
void* thread(void* arg) {
	pthread_mutex_lock(&mutex);
	// região critica
	pthread_mutex_unlock(&mutex);
	return 0;
}
int main() {
	pthread_mutex_init(&mutex, NULL);
	pthread_create(...)
	pthread_join(...)
	pthread_mutex_destroy(&mutex);
	return 0;
}

-----------------------------------

Semáforo
=> tipo abstrato de dados:
	-> contador inteiro sem sinal
	-> fila de threads bloqueadas que esperam para acessar a região protegida pelo semáforo
=> oeprações básicas em s:
	P(s) / Down(s) = solicita acesso à região crítica
	V(s) / Up(s)   = libera região crítica
	-> ambas atômicas
	-> implementações de semáforo usam spin-locks para garantir atomicidade nas funções

=> P(s):	= sem_wait
	se (valor de s > 0):
		s--;
	senao:
		bloqueia thread e a insere no fim da fila do semaforo s (bloqueia thread)
=> V(s):	= sem_post
	se (fila de s vazia):
		s++;
	senao:
		libera thread do inicio da fila de s

=> tipos:
	-> binário: assume somente valores 0 e 1
		-> funciona como um mutex
		-> usado para implementar exclusão mútua
	-> contador: n >= 0:
		-> n indica quantas threads podem "passar" pelo semáforo em um instante
		-> sincronização entre threads

---> sem_init(sem, pshared, value)
	-> sem: do tipo sem_t
	-> pshared: flag para indicar se semáforo deve ser compartilhado entre threads (=0) ou procesos (!= 0)
	-> value: valor inicial do semáforo
---> sem_destroy(sem)

---> sem_wait(sem)	=> antes da região crítica
---> sem_post		=> depois da seção crítica

#include <semaphore.h>
sem_t semaphore;

void* thread(void* arg) {
	sem_wait(&semaphore);
	// seção crítica
	sem_post(&semaphore);
	pthread_exit(NULL);
}

int main(...) {
	sem_init(&semaphore, 0, 1);	// semáforo binário
	// cria threads + joins
	sem_destroy(&semaphore);
	return 0;
}

-----------------------------------

Deadlocks

=> mecanismos de controle de concorrência: garantem integridade dos recursos compartilhados
=> bloqueadno acesso e deixando tarefas em espera, pode comprometer "liveness"/vivacidade da app
	-> garante que execução das aplicações concorrentes será concluída em algum momento (mesmo esperando temporariamente por algum recurso)
	-> comprometida se tarefas entrarem em deadlock

- Recursos: algo que pode ser adquirido, usado e liberado por quem o manipula
	-> "hardware, arquivo, variável compartilhada protegida por mutex, ..."
=> acesso de modo:
	-> compartilhado: várias tarefas podem acessar ao mesmo sem tempo sem falha
		-> ler arquivo/dado, saída de áudoi, microfone
	-> exclusivo: só pode ser acessado por uma tarefa por vez
		-> escrever em arquivo/dado, scanner, impressora, ...
		-> acesso deve ser controlado com mec. de exclusão mútua
=> recurso preemtível: pode ser retirado do atual usuário/proprietário sem prejuízo (CPU) 
   recurso não preemptível: não pode ser retirado sem falha (gravador de CD)
	-> deadlocks nesses, normalmente
=> eventos necessários para uso de recurso:
	1. requisitar recurso: caso não disponível, solicitante espera
	2. usar o recurso: após confirmar requisção, realiza a operação
	3. liberar o recurso: libera para atender outros solicitantes

=> "um conjunto de processos está em deadklock se cada um deles está bloqueado à espera de um evento que somente outro processo desse conjunto pode fazer acontecer"

- Condições para ocorrência de deadlock
1. exclusão mútua: determinado instante, recurso está em uma das duas situações: associado a uma tarefa única ou disponível
2. posse e espera: tarefas que retem recursos concedidos anteriormente podem requisitar novos recursos
3. não preempção: recursos concedidos a uma tarefa não podem ser tomados dela (somente liberados)
4. espera circular: encadeamente circular de 2 ou mais tarefas


- Estratégias para lidar com impasses
=> ignorar por completo: "algoritmo do avestruz"
=> detecção e recuperação de impasses: tenta detectar impasses em tempo de execução (ciclos no grafo gerado pelas tarefas e recursos)
	-> após detecção, recupera sistema
	-> pode fazer preempção temporária, i.e.
=> prevenção de impasses: garantir pelo menos uma das quatro condições de deadlock não ocorra
	-> impedir ex. mútua ou posse e espera ou não preempção ou espera circular
	
	=> ex. mútua:
		impedir: que mais de uma tarefa possa requerer uso exclusivo de um recurso
		ex.: spooler de impressão (único processo do SO que pode requisitar usar impressora, nunca requisita outro recurso)
	=> posse e espera
		impedir: tarefas que possuam posso de recursos esperar por mais recursos
		ex.: exigir que todas tarefas requisitem todos recursos antes de inicializar a execução; não aloca nenhum recurso se um ou mais já estiverem sido alocados; aguarda até todos recursos necessários estarem livres
	=> não preempção:
		impedir: recurso possa tomar um recurso de outro processo
		ex.: difícil de ser implementado se dar erro
	=> espera circular:
		exigir: tarefa tenha permissão para possuir somento um recurso de cada vez (se necessário outro, liberar o primeiro antes) ou ordem global para requisitar recursos (tarefas podem resquisitar mais de um, mas sempre consideradno uma ordem numérica global de recursos)
		-> caso de ordem global: n recursos em ordem crescente; tarefas só podem requisitar um novo Rx se x for maior que o número de todos os demais recursos que a tarefa já possui ou esteja requisitando
		
- Outras questões com deadlocks
=> impasses de comunicação: dois ou mais processos se comunicam em rede e as mensagens são perdidas
=> livelocks: tarefas não ficam bloqueadas aguardando recurso, mas realizam operações sem progredir na execução
	-> solução que permite preempção de recursos pode causar livelocks se mal implementada
=> starvation (inanição): tarefa nunca consegue pegar recurso, pois outra faz antes
	-> pode ser eliminada com FIFO
		
